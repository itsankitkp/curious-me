{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc4ab4b-940b-45ad-8e6a-701e206db80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from curious_me import Curious\n",
    "from langchain_openai import ChatOpenAI\n",
    "from getpass import getpass\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38257a46-41fc-4809-a6b0-5004a461c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key ········\n"
     ]
    }
   ],
   "source": [
    "api_key = getpass('Enter API key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d7a89a-2e4c-4b29-9514-55591d349438",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            model_name=\"grok-2-1212\",\n",
    "            temperature=0.1,\n",
    "            base_url=\"https://api.x.ai/v1\",\n",
    "            api_key=api_key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fa01190-281b-47ac-9951-f0ec27f5171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['gpt','transformer', 'llm', 'RAG', 'ReLU', 'leaky ReLU', 'activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d8e257-f688-4749-8d00-12e09301d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "curious = Curious(topics=topics, llm=llm, skip_search=True) # set skip_search=False if running this first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0b40c8b-b0ae-42dd-b788-1b3b2fb611f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = curious.ask(\"How can I improve RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b81eed96-9dce-4428-9cb8-4ac3758fb12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To improve Retrieval-Augmented Generation (RAG), several strategies can be derived from the current state of research as outlined in the literature review. Here is a structured approach to enhancing RAG, based on the synthesis of findings and identified research gaps:\n",
       "\n",
       "### 1. **Enhance Retrieval Mechanisms**\n",
       "- **Improve Retrieval Efficiency:** Research indicates that enhancing the retrieval component of RAG can significantly boost overall performance. According to [2410.12837v1], focusing on retrieval efficiency can lead to better contextual information retrieval, which is crucial for the accuracy of the generative output.\n",
       "- **Handle Unstructured Information:** A noted gap in RAG research is the handling of ambiguous or unstructured data [2410.12837v1]. Developing algorithms that can effectively process and utilize such information could improve the robustness of RAG systems.\n",
       "\n",
       "### 2. **Optimize Integration of Retrieval and Generation**\n",
       "- **Coherent Integration:** The literature highlights the challenge of integrating retrieval and generation in a coherent manner [2410.12837v1]. Future improvements should focus on seamless integration to ensure that the retrieved information is effectively used in the generation process.\n",
       "- **Address Methodological Limitations:** The disjoint study of retrieval and generation components is a methodological limitation [2412.17031v1]. A more holistic approach that considers the interdependencies between these modules could lead to better performance.\n",
       "\n",
       "### 3. **Expand Domain-Specific Applications**\n",
       "- **Domain-Specific Contexts:** RAG has shown potential in specialized areas like legal, medical, and low-resource language applications [2410.12837v1]. Further research into domain-specific contexts can enhance RAG's applicability and effectiveness in these areas.\n",
       "- **Real-World Applicability:** There is a need for studies that align RAG with real-world contexts to enhance its practical utility [2412.17031v1]. This involves testing RAG in diverse practical settings to ensure its performance aligns with real-world needs.\n",
       "\n",
       "### 4. **Address Challenges in Coherence and Interpretability**\n",
       "- **Improve Coherence:** Ongoing challenges in RAG include maintaining coherence in the generated text [2410.12837v1]. Techniques such as fine-tuning the generative model with coherence-focused objectives could be beneficial.\n",
       "- **Enhance Interpretability:** The interpretability of RAG outputs is another area for improvement. Developing methods to explain how retrieved information influences the generated text can increase trust and usability in practical applications.\n",
       "\n",
       "### 5. **Consider Scalability and Computational Overhead**\n",
       "- **Scalability:** As RAG systems grow in complexity, ensuring scalability becomes crucial. Research should explore ways to scale RAG systems without compromising performance [2410.12837v1].\n",
       "- **Reduce Computational Overhead:** Addressing the computational overhead associated with RAG is essential for its practical deployment. Optimizing algorithms and hardware utilization can help mitigate this issue.\n",
       "\n",
       "### 6. **Future Research Directions**\n",
       "- **Robustness and Scope:** Future research should focus on improving the robustness of RAG models and expanding their scope of application [2410.12837v1]. This includes exploring new domains and tasks where RAG can be beneficial.\n",
       "- **Societal Implications:** Considering the societal implications of RAG, such as ethical considerations and potential biases, is crucial for its responsible development and deployment [2410.12837v1].\n",
       "\n",
       "### Limitations and Considerations\n",
       "While these strategies are based on current research, it is important to acknowledge the limitations in the available evidence. The literature suggests that while RAG has shown significant progress, there are still unresolved challenges, particularly in handling unstructured information and real-world applicability [2410.12837v1, 2412.17031v1]. Additionally, the methodological limitations, such as the disjoint study of retrieval and generation components, may impact the generalizability of findings.\n",
       "\n",
       "By addressing these areas, RAG can be improved to offer more accurate, reliable, and practical solutions across various domains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ca30d5-70dc-4f14-9510-575d3ef76224",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = curious.get_review(\"RAG\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ab889b7-b431-4601-b358-e3f69698054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1. Executive Summary\n",
       "\n",
       "The core research question revolves around the advancements and challenges in Retrieval-Augmented Generation (RAG) models within natural language processing. Recent developments in RAG have focused on enhancing retrieval efficiency, addressing biases, and expanding applications across various domains. Key findings indicate that while RAG models improve the accuracy and reliability of language model outputs, significant challenges remain in scalability, bias mitigation, and ensuring transparency.\n",
       "\n",
       "### 2. Background and Context\n",
       "\n",
       "**Historical Development:**\n",
       "Retrieval-Augmented Generation (RAG) has evolved as a critical approach for large language models (LLMs) to handle knowledge-intensive tasks [2410.07176v1]. Initially developed to address the knowledge limitations of LLMs, RAG integrates external information to enhance model performance [2410.07176v1]. The field has seen significant growth, with recent surveys providing comprehensive overviews of RAG's evolution, key components, and future directions [2410.12837v1].\n",
       "\n",
       "**Key Theoretical Frameworks:**\n",
       "RAG models operate on the principle of augmenting language generation with retrieved information, which is crucial for tasks requiring up-to-date or specialized knowledge [2410.12837v1]. Theoretical frameworks often focus on the integration of retrieval mechanisms with generation processes, emphasizing the importance of efficient retrieval and effective use of retrieved information [2407.19994v3].\n",
       "\n",
       "**Critical Definitions and Concepts:**\n",
       "RAG involves the use of external knowledge sources to enhance the output of language models. Key concepts include retrieval efficiency, context alignment, and the mitigation of biases in retrieved information [2410.12837v1, 2412.15404v1]. The term \"Advanced RAG\" refers to models that incorporate enhanced features and configurations to improve performance [2407.19994v3].\n",
       "\n",
       "### 3. Main Body Analysis\n",
       "\n",
       "**Detailed Synthesis of Research Findings:**\n",
       "Several studies have highlighted the effectiveness of RAG in improving the accuracy and reliability of LLM outputs [2410.07176v1, 2412.15404v1]. For instance, the Astute RAG model has been shown to resolve knowledge conflicts, thereby enhancing trustworthiness [2410.07176v1]. In healthcare, RAG models like MKRAG have improved medical question-answering by integrating external medical knowledge [2412.15404v1].\n",
       "\n",
       "**Integration of Multiple Perspectives:**\n",
       "RAG's application spans various domains, including healthcare, legal, and education, demonstrating its versatility [2412.15404v1]. However, the success of RAG is influenced by broader factors than previously thought, suggesting the need for more nuanced understanding of context usage [2412.17031v1].\n",
       "\n",
       "**Clear Delineation of Agreements and Contradictions:**\n",
       "While many studies agree on the benefits of RAG in enhancing LLM performance [2410.07176v1, 2412.15404v1], there are contradictions regarding the generalizability of findings across different tasks. For example, insights from claim verification tasks may not directly apply to question-answering tasks [2412.17031v1].\n",
       "\n",
       "**Critical Evaluation of Methodologies Used:**\n",
       "Methodologies in RAG research vary, with some studies using synthetic datasets to identify mechanistic components of context usage [2412.17031v1]. However, the reliance on custom test sets and the use of LLMs for evaluation introduce subjectivity and potential biases [2412.15404v1]. The lack of standardized ground truth for evaluating RAG outputs is a noted limitation [2412.15404v1].\n",
       "\n",
       "### 4. Research Gaps and Future Directions\n",
       "\n",
       "**Identify Understudied Areas:**\n",
       "There is a need for more research on the generalizability of RAG findings across different tasks and domains [2412.17031v1]. Additionally, the impact of prompt design on RAG performance requires further investigation [2412.15404v1].\n",
       "\n",
       "**Highlight Methodological Limitations:**\n",
       "The current methodologies, such as the use of synthetic datasets and custom test sets, limit the applicability of findings [2412.17031v1, 2412.15404v1]. The absence of standardized evaluation metrics also poses a challenge [2412.15404v1].\n",
       "\n",
       "**Suggest Promising Research Directions:**\n",
       "Future research should focus on developing standardized evaluation frameworks for RAG models [2412.15404v1]. Additionally, exploring the robustness of RAG models across different contexts and improving the transparency of retrieval processes are crucial [2410.12837v1].\n",
       "\n",
       "### 5. Conclusion\n",
       "\n",
       "The major themes in RAG research include the enhancement of LLM performance through external knowledge integration, the application of RAG across various domains, and the ongoing challenges related to scalability, bias, and evaluation. The overall state of research indicates significant progress in understanding and applying RAG, yet methodological limitations and the need for standardized evaluation persist. The implications for theory and practice are profound, as RAG continues to evolve and address complex information-seeking tasks across diverse fields."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d11a508-6755-423a-ad40-3273176f79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = curious.get_citation(claim=\"Leaky ReLU is better than ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7162d73-3bba-4246-a1ff-bf0ee9440738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Analysis of how the evidence supports or challenges the claim:**\n",
       "\n",
       "The provided excerpts from [2409.09981v2] offer substantial support for the claim that Leaky ReLU is better than ReLU. The evidence primarily focuses on the advantages of Leaky ReLU in addressing the limitations of ReLU, particularly the \"dying ReLU\" problem.\n",
       "\n",
       "1. **Addressing the Dying ReLU Problem:** The excerpts explicitly state that Leaky ReLU is designed to address the dying ReLU problem, where neurons fail to learn during backpropagation due to negative inputs being completely blocked by ReLU. Leaky ReLU introduces a small slope (α) for negative inputs, allowing the neuron to continue learning even when the input is negative. This is a direct improvement over ReLU, which does not handle negative inputs well.\n",
       "\n",
       "2. **Performance Metrics:** The excerpts also provide empirical evidence of Leaky ReLU's superiority over ReLU in certain performance metrics. Specifically, it is mentioned that Leaky ReLU slightly outperforms ReLU in σRMS and σ68, as shown in table 4 of the paper. This suggests that Leaky ReLU can lead to better model performance in terms of these metrics.\n",
       "\n",
       "3. **Design and Functionality:** Leaky ReLU retains all the advantages of ReLU while adding the capability to handle negative inputs. The function and its derivative are clearly defined, showing how Leaky ReLU modifies the behavior of ReLU to prevent the dying ReLU problem.\n",
       "\n",
       "**Direct quotes from papers:**\n",
       "\n",
       "- \"Leaky ReLU [53] is an improved version of ReLU, specifically designed to address the dying ReLU problem (neuron weights not changing) and has all the advantages of ReLU.\" [2409.09981v2]\n",
       "- \"However, ReLU is not centred at 0 and does not handle negative inputs well. This can cause problems during training. Leaky ReLU (section 2.2.5) is an extension of ReLU that addresses these issues.\" [2409.09981v2]\n",
       "- \"In our study, we can see that Leaky ReLU slightly outperforms ReLU in σRMS and σ68, as shown in table 4.\" [2409.09981v2]\n",
       "\n",
       "**Clear citations for all evidence:**\n",
       "\n",
       "All evidence provided is from the paper [2409.09981v2].\n",
       "\n",
       "**Assessment of the strength of evidence:**\n",
       "\n",
       "The strength of the evidence supporting the claim that Leaky ReLU is better than ReLU is robust. The excerpts provide both theoretical justification and empirical evidence for the superiority of Leaky ReLU. The theoretical advantage is clearly articulated in the design of Leaky ReLU to address the dying ReLU problem, and the empirical evidence from the study's performance metrics further supports this claim. However, it should be noted that the excerpts do not provide a comprehensive comparison across all possible scenarios or datasets, which might limit the generalizability of the findings. Nonetheless, within the context of the provided study, the evidence strongly supports the claim that Leaky ReLU is better than ReLU."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33a5f7-bd5f-4a1e-8eee-f87e82e38efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
