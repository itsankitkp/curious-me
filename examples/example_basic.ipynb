{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cc4ab4b-940b-45ad-8e6a-701e206db80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from curious_me import Curious\n",
    "from langchain_openai import ChatOpenAI\n",
    "from getpass import getpass\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38257a46-41fc-4809-a6b0-5004a461c062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter API key ········\n"
     ]
    }
   ],
   "source": [
    "api_key = getpass('Enter API key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5d7a89a-2e4c-4b29-9514-55591d349438",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "            model_name=\"grok-2-1212\",\n",
    "            temperature=0.1,\n",
    "            base_url=\"https://api.x.ai/v1\",\n",
    "            api_key=api_key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fa01190-281b-47ac-9951-f0ec27f5171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['gpt','transformer', 'llm', 'RAG', 'ReLU', 'leaky ReLU', 'activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93d8e257-f688-4749-8d00-12e09301d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "curious = Curious(topics=topics, llm=llm, skip_search=True, rebuild_vec_store=False) # set skip_search=False if running this first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b40c8b-b0ae-42dd-b788-1b3b2fb611f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = curious.ask(\"Shortcoming of gpt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81eed96-9dce-4428-9cb8-4ac3758fb12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The user's request is for a summary of the shortcomings of Generative Pre-trained Transformers (GPTs). Based on the literature review provided, I will structure the response to directly address this request, focusing on the key limitations identified across different domains, particularly in robotics and language processing.\n",
       "\n",
       "### Shortcomings of Generative Pre-trained Transformers (GPTs)\n",
       "\n",
       "**1. High Computational Demands:**\n",
       "   - One of the most significant shortcomings of GPTs is their high computational requirements. Studies indicate that deploying GPTs in robotics necessitates substantial resources, often requiring offboard wireless control due to the computational intensity of these models [2411.16917v1]. This is a recurring theme across multiple studies, highlighting the resource-intensive nature of GPTs [2411.16917v1, 2408.02479v1].\n",
       "\n",
       "**2. Limited Robustness in Behavioral Outputs:**\n",
       "   - In the context of robotics, GPTs struggle with achieving robust autonomy. The literature suggests that while these models have advanced in language processing, their application in robotics is limited by their inability to produce consistent and reliable behavioral outputs with limited resources [2411.16917v1]. This contrasts with the efficiency of biological systems, such as insect brains, which achieve robust autonomy with far fewer resources [2411.16917v1].\n",
       "\n",
       "**3. Extensive Training Data Requirements:**\n",
       "   - GPTs rely heavily on large-scale pre-training and fine-tuning processes, which necessitate extensive training data. This reliance on vast datasets poses challenges in terms of data availability and training efficiency [2205.10660v1]. The need for such large datasets is a critical limitation, particularly when considering the application of GPTs in domains where data might be scarce or difficult to obtain.\n",
       "\n",
       "**4. Resource Intensity of Methodologies:**\n",
       "   - The methodologies used in developing GPTs, such as large-scale pre-training and fine-tuning, are effective for improving language processing capabilities [2408.02479v1]. However, these methods are criticized for their resource intensity, which poses a challenge in achieving robust behavioral outputs in robotics [2411.16917v1]. The high computational demands and extensive training times are significant limitations that need to be addressed [2411.16917v1].\n",
       "\n",
       "**5. Limited Applicability Across Domains:**\n",
       "   - While GPTs have shown significant advancements in language processing, their application in other domains, such as robotics, remains challenging. There is a dichotomy in the literature, with some studies celebrating the progress in language understanding [2408.02479v1], while others question the practical utility of these models in robotics due to their high computational costs and limited robustness [2411.16917v1].\n",
       "\n",
       "### Limitations in the Available Evidence\n",
       "\n",
       "The literature review provides a comprehensive overview of the shortcomings of GPTs, but it is important to acknowledge that the evidence is primarily focused on the domains of language processing and robotics. There may be other domains where GPTs face different or additional challenges that are not covered in the provided literature. Additionally, the studies cited are primarily critical of the current state of GPTs, and there may be ongoing research that addresses some of these limitations.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In summary, the main shortcomings of GPTs include high computational demands, limited robustness in behavioral outputs, extensive training data requirements, resource-intensive methodologies, and limited applicability across domains. These limitations highlight the need for future research to focus on developing more efficient training algorithms, exploring bio-inspired approaches, and addressing the challenges of applying GPTs in diverse fields beyond language processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8ca30d5-70dc-4f14-9510-575d3ef76224",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = curious.get_review(\"feedforward networks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab889b7-b431-4601-b358-e3f69698054c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### 1. Executive Summary\n",
       "\n",
       "The core research question revolves around the understanding and application of feedforward neural networks (FNNs), focusing on their approximation capabilities, theoretical frameworks, and recent developments. The field has seen significant advancements in understanding FNNs as universal approximators and exploring their interpretability through various theoretical lenses. Key findings indicate that FNNs can approximate any continuous function with high accuracy, yet challenges remain in understanding their internal mechanisms and optimizing their architectures.\n",
       "\n",
       "### 2. Background and Context\n",
       "\n",
       "Historically, the development of FNNs can be traced back to the late 1980s, with seminal works by Cybenko [2409.14248v3] and Hornik et al. [2409.14248v3, 2411.17932v1, 2004.06632v1] establishing their universal approximation capabilities. These early findings laid the groundwork for subsequent research into the theoretical underpinnings of FNNs. Key theoretical frameworks include the universal approximation theorem, which posits that a feedforward network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of R^n [2409.14248v3, 2305.18460v3]. Critical concepts in this area include activation functions, network depth and width, and the role of training algorithms in achieving desired approximations.\n",
       "\n",
       "### 3. Main Body Analysis\n",
       "\n",
       "**Recent Developments and Historical Context:**\n",
       "Recent developments in FNNs have focused on enhancing their interpretability and understanding their approximation capabilities. For instance, [2411.17932v1] explores the use of distance metrics as a framework for interpreting neural networks, suggesting a shift from traditional intensity-based interpretations. Historically, the work of Cybenko [2409.14248v3] and Hornik et al. [2409.14248v3, 2411.17932v1, 2004.06632v1] has been pivotal in establishing the theoretical foundation for FNNs as universal approximators.\n",
       "\n",
       "**Key Theoretical Frameworks:**\n",
       "The universal approximation theorem remains a cornerstone of FNN theory, supported by multiple studies [2409.14248v3, 2305.18460v3, 2411.17932v1]. This theorem has been extended to consider the impact of different activation functions and network architectures [2004.06632v1]. Additionally, the exploration of distance metrics as a framework for understanding FNNs [2411.17932v1] represents a novel theoretical approach to interpreting these networks.\n",
       "\n",
       "**Major Findings and Contradictions:**\n",
       "Several studies have confirmed that FNNs can approximate any continuous function with high accuracy [2409.14248v3, 2305.18460v3, 2411.17932v1]. However, there are contradictions regarding the optimal architecture for achieving such approximations. While some research suggests that deeper networks may offer advantages in certain scenarios [2411.09961v5], others argue that the width of the network is more critical [2305.18460v3]. Additionally, the interpretability of FNNs remains a contentious issue, with some advocating for distance-based interpretations [2411.17932v1] and others focusing on adversarial robustness [2411.17932v1].\n",
       "\n",
       "**Methodological Evaluation:**\n",
       "The methodologies used in these studies vary, with some employing theoretical proofs [2409.14248v3, 2305.18460v3] and others relying on empirical experiments [2411.09961v5]. The use of theoretical approaches has been instrumental in establishing the universal approximation theorem, while empirical studies have provided insights into practical applications and limitations of FNNs. However, the reliance on specific datasets and the lack of standardized benchmarks can limit the generalizability of findings.\n",
       "\n",
       "### 4. Research Gaps and Future Directions\n",
       "\n",
       "**Research Gaps:**\n",
       "Despite the extensive research on FNNs, several gaps remain. The interpretability of FNNs, particularly in understanding how distance metrics can enhance this understanding, requires further exploration [2411.17932v1]. Additionally, the optimal balance between network depth and width for specific approximation tasks is not fully understood [2305.18460v3, 2411.09961v5]. The impact of different activation functions on approximation capabilities also warrants further investigation [2004.06632v1].\n",
       "\n",
       "**Future Directions:**\n",
       "Future research should focus on developing more interpretable models by integrating distance metrics and other novel frameworks [2411.17932v1]. Exploring the trade-offs between network depth and width in various applications could provide insights into optimal architectures [2305.18460v3, 2411.09961v5]. Additionally, investigating the role of different activation functions in enhancing approximation capabilities could lead to more efficient FNNs [2004.06632v1].\n",
       "\n",
       "### 5. Conclusion\n",
       "\n",
       "The literature on feedforward neural networks has established their capability as universal approximators, supported by multiple studies [2409.14248v3, 2305.18460v3, 2411.17932v1]. However, the field continues to grapple with issues of interpretability and optimal architecture design. The integration of distance metrics and the exploration of network depth versus width represent promising avenues for future research. Overall, while significant progress has been made, the complexity of FNNs necessitates ongoing research to fully harness their potential in various applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d11a508-6755-423a-ad40-3273176f79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations = curious.get_citation(claim=\"Leaky ReLU is better than ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7162d73-3bba-4246-a1ff-bf0ee9440738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Analysis of how the evidence supports or challenges the claim:**\n",
       "\n",
       "The provided excerpts from [2409.09981v2] offer substantial evidence supporting the claim that Leaky ReLU is better than ReLU. The primary advantage of Leaky ReLU over ReLU, as highlighted in the excerpts, is its ability to address the \"dying ReLU problem.\" This problem occurs when neurons in a neural network using ReLU as an activation function become inactive and fail to learn during backpropagation due to negative inputs. Leaky ReLU introduces a small slope for negative inputs, which helps prevent neurons from becoming completely inactive.\n",
       "\n",
       "The excerpts specifically mention that Leaky ReLU \"is an improved version of ReLU, specifically designed to address the dying ReLU problem\" and that it \"has all the advantages of ReLU.\" Furthermore, empirical evidence from the study cited in the excerpts shows that Leaky ReLU \"slightly outperforms ReLU in σRMS and σ68,\" indicating better performance in certain metrics.\n",
       "\n",
       "**Direct quotes from papers where relevant:**\n",
       "\n",
       "- \"Leaky ReLU [53] is an improved version of ReLU, specifically designed to address the dying ReLU problem (neuron weights not changing) and has all the advantages of ReLU.\" [2409.09981v2]\n",
       "- \"The unique feature of Leaky ReLU is the addition of the parameter α, which represents the small slope in the uj < 0 region known as the leakage in the function.\" [2409.09981v2]\n",
       "- \"In our study, we can see that Leaky ReLU slightly outperforms ReLU in σRMS and σ68, as shown in table 4.\" [2409.09981v2]\n",
       "\n",
       "**Clear citations for all evidence:**\n",
       "\n",
       "All evidence provided is from the paper [2409.09981v2].\n",
       "\n",
       "**Assessment of the strength of evidence:**\n",
       "\n",
       "The strength of the evidence supporting the claim that Leaky ReLU is better than ReLU is strong. The excerpts provide both theoretical justification and empirical evidence for the superiority of Leaky ReLU. The theoretical advantage is clearly articulated in terms of addressing the dying ReLU problem, and the empirical evidence from the study directly compares the performance of Leaky ReLU and ReLU, showing a slight advantage for Leaky ReLU.\n",
       "\n",
       "However, it is important to note that the excerpts do not provide a comprehensive comparison across all possible metrics or scenarios. The claim's validity might be limited to the specific contexts and metrics mentioned in the study. Additionally, the excerpts do not discuss potential drawbacks or scenarios where ReLU might be preferable over Leaky ReLU, which could be a limitation in fully assessing the claim's robustness."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d33a5f7-bd5f-4a1e-8eee-f87e82e38efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
